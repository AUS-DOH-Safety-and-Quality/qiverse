% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/snowflake.R
\name{ingest_dataflow_table}
\alias{ingest_dataflow_table}
\title{Ingest a PowerBI Dataflow table into Snowflake}
\usage{
ingest_dataflow_table(
  con,
  pbi_workspace,
  pbi_dataflow,
  pbi_table,
  database_name,
  schema_name,
  table_name,
  pbi_tk
)
}
\arguments{
\item{con}{The connection established by the \code{snowflake_con()} function.}

\item{pbi_workspace}{The name of the PowerBI workspace containing the dataflow}

\item{pbi_dataflow}{The name of the PowerBI dataflow containing the target table}

\item{pbi_table}{The name of the PowerBI dataflow table to ingest}

\item{database_name}{The database where the table will be stored.}

\item{schema_name}{The schema for the table to be stored.}

\item{table_name}{The name of the table for Snowflake}

\item{pbi_tk}{The token generated with the correct PowerBI permissions.
Use \code{get_az_tk("pbi_df")} to create this token.}
}
\value{
A data.frame with output metadata on the ingestion process.
}
\description{
This function uses Snowflake's Azure Blob Storage connectivity to
directly import the dataflow table, without needing to first download the
file locally and stage.
}
\examples{
 \dontrun{
# Create Snowflake azure token
tk <- get_az_tk('sf')
con <- snowflake_con(token = tk)


 <- get_az_tk('pbi_df')

# Ingest Example To Snowflake
ingest_dataflow_table(
  con = con,
  pbi_workspace = 'My PBI Workspace',
  pbi_dataflow = 'My PBI Dataflow',
  pbi_table = 'MY PBI Table',
  database_name = 'EXAMPLE_DATASET',
  schema_name = 'EXAMPLE',
  table_name = 'TEST',
  pbi_tk = pbi_tk
)
}
}
